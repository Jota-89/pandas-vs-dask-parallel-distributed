"""
DAG 3: PROCESAMIENTO PARALELO CON DASK
======================================
PROP√ìSITO: Procesar datos NYC Taxi usando Dask (procesamiento paralelo)
ORDEN: TERCERO - despu√©s del procesamiento secuencial
"""

from datetime import datetime, timedelta
import json
from pathlib import Path
import time

from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'data_team',
    'depends_on_past': False,
    'start_date': datetime(2024, 11, 12),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}

dag = DAG(
    'dag_03_dask_simple',
    default_args=default_args,
    description='DAG 3: Procesamiento paralelo con Dask',
    schedule_interval=None,
    catchup=False,
)


def parallel_processing_dask(**context):
    """DAG 3: Procesamiento paralelo con Dask - AN√ÅLISIS COMPLETO COMO SECUENCIAL"""
    print("DAG 3: PROCESAMIENTO PARALELO CON DASK - AN√ÅLISIS COMPLETO")
    print("=" * 65)

    data_dir = Path("/workspace/data/nyc_taxi")
    if not data_dir.exists():
        raise Exception("No hay datos! Ejecuta dag_01_download_data primero")

    # Usar Dask para procesamiento paralelo
    import dask.dataframe as dd
    import pandas as pd
    import numpy as np

    start_time = time.time()

    # Leer todos los archivos con Dask
    parquet_files = list(data_dir.glob("*.parquet"))
    print(f"üîç Archivos encontrados: {len(parquet_files)}")

    if not parquet_files:
        raise Exception("No hay archivos .parquet para procesar")

    # Leer con Dask (procesamiento paralelo autom√°tico) - solo columnas necesarias
    columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', 'fare_amount', 'total_amount',
               'PULocationID', 'DOLocationID', 'payment_type', 'tip_amount', 'passenger_count']

    print("üìä Cargando datos con Dask...")
    ddf = dd.read_parquet(str(data_dir / "*.parquet"), columns=columns)

    print("üîÑ Realizando an√°lisis paralelo con Dask...")

    # ===== AN√ÅLISIS COMPLETO CON DASK =====

    # 1. Estad√≠sticas b√°sicas
    total_rows = len(ddf)
    avg_fare = ddf['fare_amount'].mean()
    avg_distance = ddf['trip_distance'].mean()
    std_distance = ddf['trip_distance'].std()

    # 2. Percentiles (m√°s complejos en Dask)
    fare_percentiles = ddf['fare_amount'].quantile([0.25, 0.5, 0.75, 0.95])
    distance_percentiles = ddf['trip_distance'].quantile(
        [0.25, 0.5, 0.75, 0.95])

    # 3. An√°lisis temporal con Dask
    ddf['pickup_hour'] = dd.to_datetime(ddf['tpep_pickup_datetime']).dt.hour
    ddf['pickup_day_of_week'] = dd.to_datetime(
        ddf['tpep_pickup_datetime']).dt.dayofweek

    # Calcular duraci√≥n del viaje
    pickup_dt = dd.to_datetime(ddf['tpep_pickup_datetime'])
    dropoff_dt = dd.to_datetime(ddf['tpep_dropoff_datetime'])
    ddf['trip_duration_minutes'] = (
        dropoff_dt - pickup_dt).dt.total_seconds() / 60

    # An√°lisis por hora con Dask
    hourly_stats = ddf.groupby('pickup_hour').agg({
        'fare_amount': ['count', 'mean'],
        'trip_distance': 'mean',
        'trip_duration_minutes': 'mean'
    })

    # 4. An√°lisis geogr√°fico con Dask
    pickup_zones = ddf['PULocationID'].value_counts().nlargest(20)
    dropoff_zones = ddf['DOLocationID'].value_counts().nlargest(20)

    # 5. An√°lisis de tipos de pago con Dask
    payment_counts = ddf['payment_type'].value_counts()
    payment_tips = ddf.groupby('payment_type')[
        'tip_amount'].agg(['count', 'mean'])

    print("‚ö° Ejecutando computaci√≥n paralela...")

    # EJECUTAR TODAS LAS COMPUTACIONES EN PARALELO
    results_computed = dd.compute(
        total_rows, avg_fare, avg_distance, std_distance,
        fare_percentiles, distance_percentiles,
        hourly_stats, pickup_zones, dropoff_zones,
        payment_counts, payment_tips
    )

    # Extraer resultados computados
    (total_rows, avg_fare, avg_distance, std_distance,
     fare_perc, dist_perc, hourly, pickup_top, dropoff_top,
     pay_counts, pay_tips) = results_computed

    end_time = time.time()
    processing_time = end_time - start_time

    # ===== ESTRUCTURAR RESULTADOS COMO EL SECUENCIAL =====

    # An√°lisis por hora estructurado
    hourly_analysis = {}
    for hour in range(24):
        if hour in hourly.index:
            hourly_analysis[f"hour_{hour}"] = {
                'trips': int(hourly.loc[hour, ('fare_amount', 'count')]),
                'avg_fare': float(hourly.loc[hour, ('fare_amount', 'mean')]),
                'avg_distance': float(hourly.loc[hour, ('trip_distance', 'mean')]),
                'avg_duration': float(hourly.loc[hour, ('trip_duration_minutes', 'mean')])
            }

    # An√°lisis geogr√°fico estructurado
    geographic_analysis = {
        'top_pickup_zones': {f"zone_{zone_id}": int(count) for zone_id, count in pickup_top.items()},
        'top_dropoff_zones': {f"zone_{zone_id}": int(count) for zone_id, count in dropoff_top.items()}
    }

    # An√°lisis de tipos de pago estructurado
    payment_analysis = {
        'payment_distribution': {f"type_{pay_type}": int(count) for pay_type, count in pay_counts.items()},
        'tip_by_payment': {
            f"type_{pay_type}": {
                'count': int(pay_tips.loc[pay_type, 'count']),
                'avg_tip': float(pay_tips.loc[pay_type, 'mean'])
            } for pay_type in pay_tips.index
        }
    }

    # Preparar resultados completos (formato similar al secuencial)
    results = {
        'timestamp': datetime.now().isoformat(),
        'dag': 'dag_03_dask_simple',
        'method': 'dask_parallel',
        'execution_type': 'paralelo_dask_completo',
        'total_time': processing_time,
        'total_rows': int(total_rows),
        'files_processed': len(parquet_files),
        'throughput': total_rows / processing_time,
        'avg_time_per_file': processing_time / len(parquet_files),

        # An√°lisis detallado como el secuencial
        'analysis': {
            'advanced_stats': {
                'rows': int(total_rows),
                'avg_distance': float(avg_distance),
                'std_distance': float(std_distance),
                'avg_fare': float(avg_fare),
                'percentiles_distance': [float(x) for x in dist_perc.values],
                'percentiles_fare': [float(x) for x in fare_perc.values]
            },
            'hourly_analysis': hourly_analysis,
            'geographic_analysis': geographic_analysis,
            'payment_analysis': payment_analysis
        },

        # M√©tricas b√°sicas para comparaci√≥n
        'metrics': {
            'avg_fare': float(avg_fare),
            'avg_distance': float(avg_distance)
        }
    }

    # Guardar resultados
    results_dir = Path("/workspace/results")
    results_dir.mkdir(exist_ok=True)

    results_file = results_dir / "dask_simple_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)

    # Mostrar resumen completo
    print(f"\nüéØ DAG 3: COMPLETADO - Procesamiento paralelo COMPLETO")
    print(f"‚ö° Tiempo total: {processing_time:.2f}s")
    print(f"üìä Filas procesadas: {total_rows:,}")
    print(f"üöÄ Throughput: {results['throughput']:,.0f} filas/s")
    print(f"üìÅ Archivos procesados: {len(parquet_files)}")
    print(
        f"‚è±Ô∏è Tiempo promedio por archivo: {processing_time/len(parquet_files):.2f}s")

    # Mostrar algunos an√°lisis clave
    print(f"\nüìà AN√ÅLISIS COMPLETADOS:")
    print(f"   ‚Ä¢ Estad√≠sticas avanzadas (percentiles, std dev)")
    print(f"   ‚Ä¢ An√°lisis temporal por hora (24 horas)")
    print(f"   ‚Ä¢ Top 20 zonas de pickup y dropoff")
    print(f"   ‚Ä¢ An√°lisis de tipos de pago y propinas")
    print(
        f"   ‚Ä¢ M√©tricas: Tarifa ${avg_fare:.2f}, Distancia {avg_distance:.2f} mi")

    print(f"\nüìÑ Reporte completo: {results_file}")
    print(f"üîÑ Listo para comparaci√≥n con secuencial!")

    return results


# Tarea
dask_task = PythonOperator(
    task_id='parallel_processing',
    python_callable=parallel_processing_dask,
    dag=dag,
)

dag.doc_md = """
# DAG 3: Procesamiento Paralelo (Dask) - AN√ÅLISIS COMPLETO

**ORDEN: EJECUTAR TERCERO**

## Prerrequisitos
- ‚úÖ dag_01_download_data completado
- ‚úÖ dag_02_sequential completado

## M√©todo
- **Dask**: Procesamiento distribuido/paralelo autom√°tico
- **Paralelizaci√≥n**: M√∫ltiples workers procesan datos simult√°neamente
- **An√°lisis Completo**: Mismos an√°lisis que el secuencial para comparaci√≥n justa

## An√°lisis Incluidos
- ‚úÖ Estad√≠sticas avanzadas (percentiles, desviaci√≥n est√°ndar)
- ‚úÖ An√°lisis temporal por hora (24 horas)
- ‚úÖ An√°lisis geogr√°fico (top 20 zonas pickup/dropoff)
- ‚úÖ An√°lisis de tipos de pago y propinas

## Archivo Generado
- `dask_simple_results.json` (con an√°lisis completo)

## Siguiente
- ‚ñ∂Ô∏è DAG 4: Comparaci√≥n y gr√°ficos t√©cnicos
"""
