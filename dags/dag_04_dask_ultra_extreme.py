"""
ğŸš€ğŸš€ğŸš€ DAG ULTRA EXTREMO - MÃXIMA POTENCIA ABSOLUTA ğŸš€ğŸš€ğŸš€
ConfiguraciÃ³n que exprime hasta la Ãºltima gota de rendimiento del sistema
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'performance_team_ultra',
    'depends_on_past': False,
    'start_date': datetime(2025, 11, 13),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=2),
}


def parallel_processing_ultra_extreme():
    """
    ğŸ”¥ğŸ”¥ğŸ”¥ PROCESAMIENTO ULTRA EXTREMO - MÃXIMA POTENCIA ABSOLUTA ğŸ”¥ğŸ”¥ğŸ”¥
    """
    print("ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥")
    print("ğŸ’¥ğŸ’¥ğŸ’¥ ULTRA EXTREMO - Â¡EXPLOTANDO TODOS LOS LÃMITES! ğŸ’¥ğŸ’¥ğŸ’¥")
    print("ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥")

    import dask.dataframe as dd
    import time
    import json
    from distributed import Client
    import pandas as pd
    import dask

    # CONFIGURACIÃ“N ULTRA AGRESIVA
    dask.config.set({
        'array.chunk-size': '1GiB',  # Chunks gigantes
        'dataframe.shuffle.method': 'p2p',
        'distributed.worker.memory.target': 0.85,  # MÃ¡s agresivo
        'distributed.worker.memory.spill': 0.90,
        'distributed.worker.memory.pause': 0.95,
        'distributed.worker.memory.terminate': 0.98,
        'distributed.scheduler.bandwidth': 1e9,  # 1GB/s bandwidth
        'distributed.comm.compression': 'lz4',  # CompresiÃ³n rÃ¡pida
        'distributed.scheduler.allowed-failures': 10,  # MÃ¡s tolerante a fallos
        'dataframe.optimize-graph': True,
        'array.slicing.split_large_chunks': True,
        'distributed.scheduler.work-stealing': True,
        'distributed.scheduler.work-stealing-interval': '10ms'  # Ultra agresivo
    })

    start_time = time.time()

    # Conectar al cluster ULTRA
    client = Client('127.0.0.1:8786', timeout='45s')

    print(f"ğŸš€ CLUSTER ULTRA EXTREMO CONECTADO:")
    scheduler_info = client.scheduler_info()
    workers = scheduler_info.get('workers', {})
    total_cores = sum(w.get('nthreads', 0) for w in workers.values())
    total_memory_gb = sum(w.get('memory_limit', 0)
                          for w in workers.values()) / (1024**3)

    print(f"   ğŸ’¥ Workers: {len(workers)}")
    print(f"   ğŸ§  Memoria total: {total_memory_gb:.1f} GB")
    print(f"   âš¡ Cores totales: {total_cores}")
    print(f"   ğŸ”¥ ConfiguraciÃ³n: Â¡ULTRA EXTREMA!")

    # Buscar archivos de datos
    import glob
    data_files = glob.glob('/workspace/data/nyc_taxi/*.parquet')
    print(f"\nğŸ“‚ Archivos encontrados: {len(data_files)}")

    # Calcular tamaÃ±o total de los datos
    import os
    total_size_gb = sum(os.path.getsize(file)
                        for file in data_files) / (1024**3)
    print(f"ğŸ’¾ TamaÃ±o total datos: {total_size_gb:.2f} GB")

    print(f"\nğŸ’¥ğŸ’¥ğŸ’¥ INICIANDO PROCESAMIENTO ULTRA EXTREMO ğŸ’¥ğŸ’¥ğŸ’¥")
    print("âš¡ Etapa 1: Carga paralela ULTRA AGRESIVA...")

    # ConfiguraciÃ³n EXTREMA para datos masivos
    df = dd.read_parquet(
        '/workspace/data/nyc_taxi/*.parquet',
        blocksize='256MB'  # Bloques ULTRA grandes
    )

    # ReparticiÃ³n EXTREMA basada en workers disponibles
    extreme_partitions = total_cores * 12  # ULTRA AGRESIVO
    print(f"ğŸ’¥ Reparticionando en {extreme_partitions} particiones EXTREMAS...")
    df = df.repartition(npartitions=extreme_partitions)

    print("âš¡ Etapa 2: PreparaciÃ³n anÃ¡lisis ULTRA EXTREMO...")

    # AnÃ¡lisis paralelos ULTRA MASIVOS
    calculations = []

    print("âš¡ Etapa 3: Configurando computaciones ULTRA EXTREMAS...")

    # Para cada archivo, crear MÃšLTIPLES anÃ¡lisis paralelos EXTREMOS
    for i, data_file in enumerate(data_files):
        file_name = os.path.basename(data_file)
        print(f"      ğŸ’¥ Preparando EXTREMO {file_name}...")

        # Cargar archivo especÃ­fico para anÃ¡lisis ULTRA detallado
        df_month = dd.read_parquet(data_file, blocksize='128MB')

        # AnÃ¡lisis MÃšLTIPLE EXTREMO por archivo
        calculations.extend([
            # BÃ¡sicos EXTREMOS
            df_month['total_amount'].sum(),
            df_month['tip_amount'].mean(),
            df_month['passenger_count'].sum(),
            df_month.shape[0],  # count
            df_month['trip_distance'].mean(),
            df_month['fare_amount'].max(),
            df_month['fare_amount'].min(),
            df_month.dropna()['total_amount'].std(),
            df_month[df_month['passenger_count'] > 0]['fare_amount'].mean(),

            # AnÃ¡lisis ADICIONALES EXTREMOS
            df_month['tip_amount'].max(),
            df_month['trip_distance'].std(),
            df_month['passenger_count'].max(),
            df_month[df_month['trip_distance'] > 0]['trip_distance'].mean(),
            df_month['total_amount'].quantile(0.95),
            df_month['fare_amount'].quantile(0.05),
        ])

    # AnÃ¡lisis globales ULTRA ADICIONALES
    calculations.extend([
        df.shape[0],  # total count
        df['total_amount'].sum(),
        df['total_amount'].mean(),
        df['total_amount'].std(),
        df['total_amount'].max(),
        df['total_amount'].min(),
        df['tip_amount'].sum(),
        df['tip_amount'].mean(),
        df['tip_amount'].std(),
        df['tip_amount'].max(),
        df['fare_amount'].sum(),
        df['fare_amount'].mean(),
        df['fare_amount'].max(),
        df['fare_amount'].min(),
        df['fare_amount'].std(),
        df['trip_distance'].sum(),
        df['trip_distance'].mean(),
        df['trip_distance'].max(),
        df['trip_distance'].std(),
        df['passenger_count'].sum(),
        df['passenger_count'].mean(),
        df['passenger_count'].max(),
        # PERCENTILES EXTREMOS
        df['total_amount'].quantile(0.99),
        df['total_amount'].quantile(0.01),
        df['fare_amount'].quantile(0.99),
        df['fare_amount'].quantile(0.01),
        df['trip_distance'].quantile(0.99),
        df['trip_distance'].quantile(0.01),
        # FILTROS EXTREMOS
        df[df['total_amount'] > 0].shape[0],
        df[df['trip_distance'] > 0].shape[0],
        df[df['passenger_count'] > 0].shape[0],
        df[df['fare_amount'] > 0]['total_amount'].mean(),
    ])

    print(f"\nğŸ’¥ğŸ’¥ğŸ’¥ EJECUTANDO {len(calculations)} CÃLCULOS ULTRA EXTREMOS! ğŸ’¥ğŸ’¥ğŸ’¥")
    print("ğŸ”¥ Â¡USANDO ABSOLUTAMENTE TODA LA POTENCIA DEL SISTEMA!")
    print(f"âš¡ Distribuyendo trabajo EXTREMO en {len(workers)} workers...")

    # ğŸ’¥ğŸ’¥ğŸ’¥ COMPUTACIÃ“N ULTRA EXTREMA ğŸ’¥ğŸ’¥ğŸ’¥
    computation_start = time.time()
    try:
        results = dd.compute(
            *calculations,
            scheduler=client,
            resources={'memory': '800MB'},  # LÃ­mite ajustado para mÃ¡s workers
            retries=3,
            priority='high'
        )
        computation_time = time.time() - computation_start

        print(
            f"âœ…ğŸ’¥ Â¡COMPUTACIÃ“N ULTRA EXTREMA COMPLETADA EN {computation_time:.2f}s! ğŸ’¥âœ…")
        print(f"ğŸ”¥ Resultados EXTREMOS: {len(results)} cÃ¡lculos completados")
        print(
            f"âš¡ Throughput ULTRA: {len(results)/computation_time:.2f} cÃ¡lculos/segundo")

    except Exception as e:
        print(f"âŒ Error en computaciÃ³n EXTREMA: {e}")
        raise
    finally:
        client.close()

    total_time = time.time() - start_time

    # Resultados ULTRA detallados
    performance_results = {
        'dag_type': 'dask_ultra_extreme',
        'total_time_seconds': total_time,
        'computation_time_seconds': computation_time,
        'total_files_processed': len(data_files),
        'total_data_size_gb': total_size_gb,
        'total_calculations': len(calculations),
        'workers_used': len(workers),
        'total_cores': total_cores,
        'total_memory_gb': total_memory_gb,
        'partitions_created': extreme_partitions,
        'throughput_calc_per_second': len(results)/computation_time if computation_time > 0 else 0,
        'data_throughput_gb_per_second': total_size_gb/total_time if total_time > 0 else 0,
        'performance_metrics': {
            'setup_time': computation_start - start_time,
            'pure_computation_time': computation_time,
            'cleanup_time': total_time - computation_start - computation_time
        },
        'extreme_config': {
            'partitions': extreme_partitions,
            'compression': 'lz4',
            'work_stealing': True,
            'memory_target': 0.85,
            'chunk_size': '1GiB'
        }
    }

    # Guardar resultados EXTREMOS
    with open('/workspace/results/dask_ultra_extreme_results.json', 'w') as f:
        json.dump(performance_results, f, indent=2)

    print(f"\nğŸ†ğŸ’¥ğŸ’¥ğŸ’¥ PROCESAMIENTO ULTRA EXTREMO COMPLETADO ğŸ’¥ğŸ’¥ğŸ’¥ğŸ†")
    print(f"ğŸ•’ Tiempo total: {total_time:.2f} segundos")
    print(f"âš¡ Archivos procesados: {len(data_files)}")
    print(f"ğŸ”¥ Datos procesados: {total_size_gb:.2f} GB")
    print(f"ğŸ’ª Throughput: {total_size_gb/total_time:.2f} GB/s")
    print(f"ğŸ“Š CÃ¡lculos completados: {len(results)}")
    print(f"ğŸ’¥ Particiones: {extreme_partitions}")
    print("ğŸš€ğŸ’¥ Â¡ULTRA EXTREMO SUPREMACÃA ABSOLUTA! ğŸ’¥ğŸš€")

    return {
        'status': 'ULTRA_EXTREME_SUCCESS',
        'total_time': total_time,
        'files_processed': len(data_files),
        'data_size_gb': total_size_gb,
        'calculations': len(results),
        'throughput_gb_s': total_size_gb/total_time,
        'partitions': extreme_partitions,
        'workers': len(workers),
        'cores': total_cores
    }


# DAG ULTRA EXTREMO
dag = DAG(
    'dag_04_dask_ultra_extreme',
    default_args=default_args,
    description='ğŸš€ğŸ’¥ Procesamiento Dask ULTRA EXTREMO - MÃ¡xima potencia absoluta',
    schedule_interval=None,
    catchup=False,
    tags=['dask', 'ultra', 'extreme', 'maximum_performance']
)

ultra_extreme_task = PythonOperator(
    task_id='ultra_extreme_processing',
    python_callable=parallel_processing_ultra_extreme,
    dag=dag
)

ultra_extreme_task
